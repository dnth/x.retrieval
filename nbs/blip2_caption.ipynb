{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xretrieval\n",
    "\n",
    "xretrieval.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xretrieval.load_dataset(\"coco-val-2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models(\"blip2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xinfer.create_model(\"Salesforce/blip2-opt-2.7b\", device=\"cuda\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions_batch(examples):\n",
    "    image_paths = [filename for filename in examples['image_path']]\n",
    "    prompts = [\"\"] * len(image_paths) \n",
    "    \n",
    "    results = model.infer_batch(image_paths, prompts, max_new_tokens=20)\n",
    "    captions = [res.text for res in results]\n",
    "\n",
    "    examples['caption'] = captions\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip2_captioned_dataset = dataset.map(generate_captions_batch, \n",
    "                                      batched=True, \n",
    "                                      batch_size=300, \n",
    "                                      desc=\"Generating captions\")\n",
    "blip2_captioned_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = blip2_captioned_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139</td>\n",
       "      <td>000000000139.jpg</td>\n",
       "      <td>data/coco/val2017/000000000139.jpg</td>\n",
       "      <td>a living room with a fireplace and a table</td>\n",
       "      <td>book,chair,clock,dining table,microwave,person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285</td>\n",
       "      <td>000000000285.jpg</td>\n",
       "      <td>data/coco/val2017/000000000285.jpg</td>\n",
       "      <td>a brown bear sitting in the grass</td>\n",
       "      <td>bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>000000000632.jpg</td>\n",
       "      <td>data/coco/val2017/000000000632.jpg</td>\n",
       "      <td>a bedroom with a bed, a dresser, a bookcase an...</td>\n",
       "      <td>bed,book,chair,potted plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724</td>\n",
       "      <td>000000000724.jpg</td>\n",
       "      <td>data/coco/val2017/000000000724.jpg</td>\n",
       "      <td>a stop sign on a street corner</td>\n",
       "      <td>car,stop sign,truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>776</td>\n",
       "      <td>000000000776.jpg</td>\n",
       "      <td>data/coco/val2017/000000000776.jpg</td>\n",
       "      <td>a group of three teddy bears</td>\n",
       "      <td>bed,teddy bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>581317</td>\n",
       "      <td>000000581317.jpg</td>\n",
       "      <td>data/coco/val2017/000000581317.jpg</td>\n",
       "      <td>a woman standing on a hill</td>\n",
       "      <td>cell phone,person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>581357</td>\n",
       "      <td>000000581357.jpg</td>\n",
       "      <td>data/coco/val2017/000000581357.jpg</td>\n",
       "      <td>a man on a skateboard</td>\n",
       "      <td>bench,person,skateboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>581482</td>\n",
       "      <td>000000581482.jpg</td>\n",
       "      <td>data/coco/val2017/000000581482.jpg</td>\n",
       "      <td>a large clock in a large building with a large...</td>\n",
       "      <td>clock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>581615</td>\n",
       "      <td>000000581615.jpg</td>\n",
       "      <td>data/coco/val2017/000000581615.jpg</td>\n",
       "      <td>a urinal in a bathroom</td>\n",
       "      <td>toilet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>581781</td>\n",
       "      <td>000000581781.jpg</td>\n",
       "      <td>data/coco/val2017/000000581781.jpg</td>\n",
       "      <td>a display of bananas and other fruits</td>\n",
       "      <td>banana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id         file_name                          image_path  \\\n",
       "0          139  000000000139.jpg  data/coco/val2017/000000000139.jpg   \n",
       "1          285  000000000285.jpg  data/coco/val2017/000000000285.jpg   \n",
       "2          632  000000000632.jpg  data/coco/val2017/000000000632.jpg   \n",
       "3          724  000000000724.jpg  data/coco/val2017/000000000724.jpg   \n",
       "4          776  000000000776.jpg  data/coco/val2017/000000000776.jpg   \n",
       "...        ...               ...                                 ...   \n",
       "4995    581317  000000581317.jpg  data/coco/val2017/000000581317.jpg   \n",
       "4996    581357  000000581357.jpg  data/coco/val2017/000000581357.jpg   \n",
       "4997    581482  000000581482.jpg  data/coco/val2017/000000581482.jpg   \n",
       "4998    581615  000000581615.jpg  data/coco/val2017/000000581615.jpg   \n",
       "4999    581781  000000581781.jpg  data/coco/val2017/000000581781.jpg   \n",
       "\n",
       "                                                caption  \\\n",
       "0            a living room with a fireplace and a table   \n",
       "1                     a brown bear sitting in the grass   \n",
       "2     a bedroom with a bed, a dresser, a bookcase an...   \n",
       "3                        a stop sign on a street corner   \n",
       "4                          a group of three teddy bears   \n",
       "...                                                 ...   \n",
       "4995                         a woman standing on a hill   \n",
       "4996                              a man on a skateboard   \n",
       "4997  a large clock in a large building with a large...   \n",
       "4998                             a urinal in a bathroom   \n",
       "4999              a display of bananas and other fruits   \n",
       "\n",
       "                                                   name  \n",
       "0     book,chair,clock,dining table,microwave,person...  \n",
       "1                                                  bear  \n",
       "2                           bed,book,chair,potted plant  \n",
       "3                                   car,stop sign,truck  \n",
       "4                                        bed,teddy bear  \n",
       "...                                                 ...  \n",
       "4995                                  cell phone,person  \n",
       "4996                            bench,person,skateboard  \n",
       "4997                                              clock  \n",
       "4998                                             toilet  \n",
       "4999                                             banana  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xretrieval\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"blip2_captioned_coco_val_2017.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       Available Models                        </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Model ID                                      </span>┃<span style=\"font-weight: bold\"> Model Input </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers/Salesforce/blip2-itm-vit-g       </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text-image  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers/Salesforce/blip2-itm-vit-g-text  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers/Salesforce/blip2-itm-vit-g-image </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> image       </span>│\n",
       "└───────────────────────────────────────────────┴─────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                       Available Models                        \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mModel ID                                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel Input\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers/Salesforce/blip2-itm-vit-g      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext-image \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers/Salesforce/blip2-itm-vit-g-text \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers/Salesforce/blip2-itm-vit-g-image\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mimage      \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────────────────────────┴─────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xretrieval.list_models(\"blip2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 12:41:28.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxretrieval.core\u001b[0m:\u001b[36mrun_benchmark\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEncoding database text for transformers/Salesforce/blip2-itm-vit-g\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e731ae88a582443d98a6ff20e487a234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding captions:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "\u001b[32m2024-11-20 12:41:29.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxretrieval.core\u001b[0m:\u001b[36mrun_benchmark\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mEncoding query text for transformers/Salesforce/blip2-itm-vit-g\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25b8260fe6c484ca467b552c5c32d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding captions:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5632f9283348ae869bbe04be60358c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.2617,\n",
       " 'NormalizedDCG': 0.2874,\n",
       " 'Precision': 0.1589,\n",
       " 'Recall': 0.3622,\n",
       " 'HitRate': 0.3622,\n",
       " 'MAP': 0.2556}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xretrieval.run_benchmark(\n",
    "    dataset=df,\n",
    "    model_id=\"transformers/Salesforce/blip2-itm-vit-g\",\n",
    "    mode=\"text-to-text\",\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 12:41:36.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxretrieval.core\u001b[0m:\u001b[36mrun_benchmark\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mEncoding database images for transformers/Salesforce/blip2-itm-vit-g\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a064c29a873545e8b798cae86eac809e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding images:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 12:42:58.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxretrieval.core\u001b[0m:\u001b[36mrun_benchmark\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mEncoding query text for transformers/Salesforce/blip2-itm-vit-g\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc76ab7a56314e4481f81c73779c118d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding captions:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.3999,\n",
       " 'NormalizedDCG': 0.5039,\n",
       " 'Precision': 0.276,\n",
       " 'Recall': 0.7264,\n",
       " 'HitRate': 0.7264,\n",
       " 'MAP': 0.3882}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xretrieval.run_benchmark(\n",
    "    dataset=df,\n",
    "    model_id=\"transformers/Salesforce/blip2-itm-vit-g\",\n",
    "    mode=\"text-to-image\",\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xretrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                    Available Datasets                     </span>\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Dataset Name  </span>┃<span style=\"font-weight: bold\"> Description                             </span>┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> coco-val-2017 </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> The COCO Validation Set with 5k images. </span>│\n",
       "└───────────────┴─────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                    Available Datasets                     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mDataset Name \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDescription                            \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mcoco-val-2017\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mThe COCO Validation Set with 5k images.\u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────┴─────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xretrieval\n",
    "\n",
    "xretrieval.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                         Available Models                         </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Model ID                                         </span>┃<span style=\"font-weight: bold\"> Model Input </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers/Salesforce/blip2-itm-vit-g          </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text-image  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers/Salesforce/blip2-itm-vit-g-text     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers/Salesforce/blip2-itm-vit-g-image    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> image       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> sentence-transformers/paraphrase-MiniLM-L3-v2    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> sentence-transformers/paraphrase-albert-small-v2 </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> sentence-transformers/multi-qa-distilbert-cos-v1 </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> sentence-transformers/all-MiniLM-L12-v2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> sentence-transformers/all-distilroberta-v1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> sentence-transformers/multi-qa-mpnet-base-dot-v1 </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> sentence-transformers/all-mpnet-base-v2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> sentence-transformers/multi-qa-MiniLM-L6-cos-v1  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> sentence-transformers/all-MiniLM-L6-v2           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm/resnet18.a1_in1k                            </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> image       </span>│\n",
       "└──────────────────────────────────────────────────┴─────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                         Available Models                         \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mModel ID                                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel Input\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers/Salesforce/blip2-itm-vit-g         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext-image \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers/Salesforce/blip2-itm-vit-g-text    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers/Salesforce/blip2-itm-vit-g-image   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mimage      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36msentence-transformers/paraphrase-MiniLM-L3-v2   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36msentence-transformers/paraphrase-albert-small-v2\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36msentence-transformers/multi-qa-distilbert-cos-v1\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36msentence-transformers/all-MiniLM-L12-v2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36msentence-transformers/all-distilroberta-v1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36msentence-transformers/multi-qa-mpnet-base-dot-v1\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36msentence-transformers/all-mpnet-base-v2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36msentence-transformers/multi-qa-MiniLM-L6-cos-v1 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36msentence-transformers/all-MiniLM-L6-v2          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm/resnet18.a1_in1k                           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mimage      \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────────────────────────────────────────┴─────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xretrieval.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 12:53:21.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxretrieval.datasets.coco\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mCOCO validation dataset found in data/coco/val2017, skipping download\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = xretrieval.load_dataset(\"coco-val-2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139</td>\n",
       "      <td>000000000139.jpg</td>\n",
       "      <td>data/coco/val2017/000000000139.jpg</td>\n",
       "      <td>A woman stands in the dining area at the table...</td>\n",
       "      <td>book,chair,clock,dining table,microwave,person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285</td>\n",
       "      <td>000000000285.jpg</td>\n",
       "      <td>data/coco/val2017/000000000285.jpg</td>\n",
       "      <td>A big burly grizzly bear is show with grass in...</td>\n",
       "      <td>bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>000000000632.jpg</td>\n",
       "      <td>data/coco/val2017/000000000632.jpg</td>\n",
       "      <td>Bedroom scene with a bookcase, blue comforter ...</td>\n",
       "      <td>bed,book,chair,potted plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724</td>\n",
       "      <td>000000000724.jpg</td>\n",
       "      <td>data/coco/val2017/000000000724.jpg</td>\n",
       "      <td>A stop sign is mounted upside-down on it's pos...</td>\n",
       "      <td>car,stop sign,truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>776</td>\n",
       "      <td>000000000776.jpg</td>\n",
       "      <td>data/coco/val2017/000000000776.jpg</td>\n",
       "      <td>Three teddy bears, each a different color, snu...</td>\n",
       "      <td>bed,teddy bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>581317</td>\n",
       "      <td>000000581317.jpg</td>\n",
       "      <td>data/coco/val2017/000000581317.jpg</td>\n",
       "      <td>A woman holding a small item in a field. Woman...</td>\n",
       "      <td>cell phone,person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>581357</td>\n",
       "      <td>000000581357.jpg</td>\n",
       "      <td>data/coco/val2017/000000581357.jpg</td>\n",
       "      <td>Skate boarder hang time with young man appeari...</td>\n",
       "      <td>bench,person,skateboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>581482</td>\n",
       "      <td>000000581482.jpg</td>\n",
       "      <td>data/coco/val2017/000000581482.jpg</td>\n",
       "      <td>A fancy metal cloth is seen against huge barre...</td>\n",
       "      <td>clock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>581615</td>\n",
       "      <td>000000581615.jpg</td>\n",
       "      <td>data/coco/val2017/000000581615.jpg</td>\n",
       "      <td>A picture of a white toilet in the rest room. ...</td>\n",
       "      <td>toilet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>581781</td>\n",
       "      <td>000000581781.jpg</td>\n",
       "      <td>data/coco/val2017/000000581781.jpg</td>\n",
       "      <td>A store display filled with ripe, unripe banan...</td>\n",
       "      <td>banana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id         file_name                          image_path  \\\n",
       "0          139  000000000139.jpg  data/coco/val2017/000000000139.jpg   \n",
       "1          285  000000000285.jpg  data/coco/val2017/000000000285.jpg   \n",
       "2          632  000000000632.jpg  data/coco/val2017/000000000632.jpg   \n",
       "3          724  000000000724.jpg  data/coco/val2017/000000000724.jpg   \n",
       "4          776  000000000776.jpg  data/coco/val2017/000000000776.jpg   \n",
       "...        ...               ...                                 ...   \n",
       "4995    581317  000000581317.jpg  data/coco/val2017/000000581317.jpg   \n",
       "4996    581357  000000581357.jpg  data/coco/val2017/000000581357.jpg   \n",
       "4997    581482  000000581482.jpg  data/coco/val2017/000000581482.jpg   \n",
       "4998    581615  000000581615.jpg  data/coco/val2017/000000581615.jpg   \n",
       "4999    581781  000000581781.jpg  data/coco/val2017/000000581781.jpg   \n",
       "\n",
       "                                                caption  \\\n",
       "0     A woman stands in the dining area at the table...   \n",
       "1     A big burly grizzly bear is show with grass in...   \n",
       "2     Bedroom scene with a bookcase, blue comforter ...   \n",
       "3     A stop sign is mounted upside-down on it's pos...   \n",
       "4     Three teddy bears, each a different color, snu...   \n",
       "...                                                 ...   \n",
       "4995  A woman holding a small item in a field. Woman...   \n",
       "4996  Skate boarder hang time with young man appeari...   \n",
       "4997  A fancy metal cloth is seen against huge barre...   \n",
       "4998  A picture of a white toilet in the rest room. ...   \n",
       "4999  A store display filled with ripe, unripe banan...   \n",
       "\n",
       "                                                   name  \n",
       "0     book,chair,clock,dining table,microwave,person...  \n",
       "1                                                  bear  \n",
       "2                           bed,book,chair,potted plant  \n",
       "3                                   car,stop sign,truck  \n",
       "4                                        bed,teddy bear  \n",
       "...                                                 ...  \n",
       "4995                                  cell phone,person  \n",
       "4996                            bench,person,skateboard  \n",
       "4997                                              clock  \n",
       "4998                                             toilet  \n",
       "4999                                             banana  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Available Models                              </span>\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Implementation </span>┃<span style=\"font-weight: bold\"> Model ID                          </span>┃<span style=\"font-weight: bold\"> Input --&gt; Output    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-opt-6.7b-coco    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-flan-t5-xxl      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-opt-6.7b         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-opt-2.7b         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> sashakunitsyn/vlrm-blip2-opt-2.7b </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "└────────────────┴───────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Available Models                              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mImplementation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel ID                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInput --> Output   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-opt-6.7b-coco   \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-flan-t5-xxl     \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-opt-6.7b        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-opt-2.7b        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35msashakunitsyn/vlrm-blip2-opt-2.7b\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "└────────────────┴───────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models(\"blip2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 12:53:25.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mModel: Salesforce/blip2-opt-2.7b\u001b[0m\n",
      "\u001b[32m2024-11-20 12:53:25.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n",
      "\u001b[32m2024-11-20 12:53:25.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDtype: float16\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f2eb47b3c548d3a5c2b27861078490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = xinfer.create_model(\"Salesforce/blip2-opt-2.7b\", device=\"cuda\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_id', 'file_name', 'image_path', 'caption', 'name'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions_batch(examples):\n",
    "    image_paths = [filename for filename in examples['image_path']]\n",
    "    prompts = [\"\"] * len(image_paths) \n",
    "    \n",
    "    results = model.infer_batch(image_paths, prompts, max_new_tokens=20)\n",
    "    captions = [res.text for res in results]\n",
    "\n",
    "    examples['caption'] = captions\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3520e37485c4167abc6803a77775e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating captions:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_id', 'file_name', 'image_path', 'caption', 'name'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blip2_captioned_dataset = dataset.map(generate_captions_batch, \n",
    "                                      batched=True, \n",
    "                                      batch_size=300, \n",
    "                                      desc=\"Generating captions\")\n",
    "blip2_captioned_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = blip2_captioned_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139</td>\n",
       "      <td>000000000139.jpg</td>\n",
       "      <td>data/coco/val2017/000000000139.jpg</td>\n",
       "      <td>a living room with a fireplace and a table</td>\n",
       "      <td>book,chair,clock,dining table,microwave,person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285</td>\n",
       "      <td>000000000285.jpg</td>\n",
       "      <td>data/coco/val2017/000000000285.jpg</td>\n",
       "      <td>a brown bear sitting in the grass</td>\n",
       "      <td>bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>000000000632.jpg</td>\n",
       "      <td>data/coco/val2017/000000000632.jpg</td>\n",
       "      <td>a bedroom with a bed, a dresser, a bookcase an...</td>\n",
       "      <td>bed,book,chair,potted plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724</td>\n",
       "      <td>000000000724.jpg</td>\n",
       "      <td>data/coco/val2017/000000000724.jpg</td>\n",
       "      <td>a stop sign on a street corner</td>\n",
       "      <td>car,stop sign,truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>776</td>\n",
       "      <td>000000000776.jpg</td>\n",
       "      <td>data/coco/val2017/000000000776.jpg</td>\n",
       "      <td>a group of three teddy bears</td>\n",
       "      <td>bed,teddy bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>581317</td>\n",
       "      <td>000000581317.jpg</td>\n",
       "      <td>data/coco/val2017/000000581317.jpg</td>\n",
       "      <td>a woman standing on a hill</td>\n",
       "      <td>cell phone,person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>581357</td>\n",
       "      <td>000000581357.jpg</td>\n",
       "      <td>data/coco/val2017/000000581357.jpg</td>\n",
       "      <td>a man on a skateboard</td>\n",
       "      <td>bench,person,skateboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>581482</td>\n",
       "      <td>000000581482.jpg</td>\n",
       "      <td>data/coco/val2017/000000581482.jpg</td>\n",
       "      <td>a large clock in a large building with a large...</td>\n",
       "      <td>clock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>581615</td>\n",
       "      <td>000000581615.jpg</td>\n",
       "      <td>data/coco/val2017/000000581615.jpg</td>\n",
       "      <td>a urinal in a bathroom</td>\n",
       "      <td>toilet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>581781</td>\n",
       "      <td>000000581781.jpg</td>\n",
       "      <td>data/coco/val2017/000000581781.jpg</td>\n",
       "      <td>a display of bananas and other fruits</td>\n",
       "      <td>banana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id         file_name                          image_path  \\\n",
       "0          139  000000000139.jpg  data/coco/val2017/000000000139.jpg   \n",
       "1          285  000000000285.jpg  data/coco/val2017/000000000285.jpg   \n",
       "2          632  000000000632.jpg  data/coco/val2017/000000000632.jpg   \n",
       "3          724  000000000724.jpg  data/coco/val2017/000000000724.jpg   \n",
       "4          776  000000000776.jpg  data/coco/val2017/000000000776.jpg   \n",
       "...        ...               ...                                 ...   \n",
       "4995    581317  000000581317.jpg  data/coco/val2017/000000581317.jpg   \n",
       "4996    581357  000000581357.jpg  data/coco/val2017/000000581357.jpg   \n",
       "4997    581482  000000581482.jpg  data/coco/val2017/000000581482.jpg   \n",
       "4998    581615  000000581615.jpg  data/coco/val2017/000000581615.jpg   \n",
       "4999    581781  000000581781.jpg  data/coco/val2017/000000581781.jpg   \n",
       "\n",
       "                                                caption  \\\n",
       "0            a living room with a fireplace and a table   \n",
       "1                     a brown bear sitting in the grass   \n",
       "2     a bedroom with a bed, a dresser, a bookcase an...   \n",
       "3                        a stop sign on a street corner   \n",
       "4                          a group of three teddy bears   \n",
       "...                                                 ...   \n",
       "4995                         a woman standing on a hill   \n",
       "4996                              a man on a skateboard   \n",
       "4997  a large clock in a large building with a large...   \n",
       "4998                             a urinal in a bathroom   \n",
       "4999              a display of bananas and other fruits   \n",
       "\n",
       "                                                   name  \n",
       "0     book,chair,clock,dining table,microwave,person...  \n",
       "1                                                  bear  \n",
       "2                           bed,book,chair,potted plant  \n",
       "3                                   car,stop sign,truck  \n",
       "4                                        bed,teddy bear  \n",
       "...                                                 ...  \n",
       "4995                                  cell phone,person  \n",
       "4996                            bench,person,skateboard  \n",
       "4997                                              clock  \n",
       "4998                                             toilet  \n",
       "4999                                             banana  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139</td>\n",
       "      <td>000000000139.jpg</td>\n",
       "      <td>data/coco/val2017/000000000139.jpg</td>\n",
       "      <td>a living room with a fireplace and a table</td>\n",
       "      <td>book,chair,clock,dining table,microwave,person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285</td>\n",
       "      <td>000000000285.jpg</td>\n",
       "      <td>data/coco/val2017/000000000285.jpg</td>\n",
       "      <td>a brown bear sitting in the grass</td>\n",
       "      <td>bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>000000000632.jpg</td>\n",
       "      <td>data/coco/val2017/000000000632.jpg</td>\n",
       "      <td>a bedroom with a bed, a dresser, a bookcase an...</td>\n",
       "      <td>bed,book,chair,potted plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724</td>\n",
       "      <td>000000000724.jpg</td>\n",
       "      <td>data/coco/val2017/000000000724.jpg</td>\n",
       "      <td>a stop sign on a street corner</td>\n",
       "      <td>car,stop sign,truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>776</td>\n",
       "      <td>000000000776.jpg</td>\n",
       "      <td>data/coco/val2017/000000000776.jpg</td>\n",
       "      <td>a group of three teddy bears</td>\n",
       "      <td>bed,teddy bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>581317</td>\n",
       "      <td>000000581317.jpg</td>\n",
       "      <td>data/coco/val2017/000000581317.jpg</td>\n",
       "      <td>a woman standing on a hill</td>\n",
       "      <td>cell phone,person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>581357</td>\n",
       "      <td>000000581357.jpg</td>\n",
       "      <td>data/coco/val2017/000000581357.jpg</td>\n",
       "      <td>a man on a skateboard</td>\n",
       "      <td>bench,person,skateboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>581482</td>\n",
       "      <td>000000581482.jpg</td>\n",
       "      <td>data/coco/val2017/000000581482.jpg</td>\n",
       "      <td>a large clock in a large building with a large...</td>\n",
       "      <td>clock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>581615</td>\n",
       "      <td>000000581615.jpg</td>\n",
       "      <td>data/coco/val2017/000000581615.jpg</td>\n",
       "      <td>a urinal in a bathroom</td>\n",
       "      <td>toilet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>581781</td>\n",
       "      <td>000000581781.jpg</td>\n",
       "      <td>data/coco/val2017/000000581781.jpg</td>\n",
       "      <td>a display of bananas and other fruits</td>\n",
       "      <td>banana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id         file_name                          image_path  \\\n",
       "0          139  000000000139.jpg  data/coco/val2017/000000000139.jpg   \n",
       "1          285  000000000285.jpg  data/coco/val2017/000000000285.jpg   \n",
       "2          632  000000000632.jpg  data/coco/val2017/000000000632.jpg   \n",
       "3          724  000000000724.jpg  data/coco/val2017/000000000724.jpg   \n",
       "4          776  000000000776.jpg  data/coco/val2017/000000000776.jpg   \n",
       "...        ...               ...                                 ...   \n",
       "4995    581317  000000581317.jpg  data/coco/val2017/000000581317.jpg   \n",
       "4996    581357  000000581357.jpg  data/coco/val2017/000000581357.jpg   \n",
       "4997    581482  000000581482.jpg  data/coco/val2017/000000581482.jpg   \n",
       "4998    581615  000000581615.jpg  data/coco/val2017/000000581615.jpg   \n",
       "4999    581781  000000581781.jpg  data/coco/val2017/000000581781.jpg   \n",
       "\n",
       "                                                caption  \\\n",
       "0            a living room with a fireplace and a table   \n",
       "1                     a brown bear sitting in the grass   \n",
       "2     a bedroom with a bed, a dresser, a bookcase an...   \n",
       "3                        a stop sign on a street corner   \n",
       "4                          a group of three teddy bears   \n",
       "...                                                 ...   \n",
       "4995                         a woman standing on a hill   \n",
       "4996                              a man on a skateboard   \n",
       "4997  a large clock in a large building with a large...   \n",
       "4998                             a urinal in a bathroom   \n",
       "4999              a display of bananas and other fruits   \n",
       "\n",
       "                                                   name  \n",
       "0     book,chair,clock,dining table,microwave,person...  \n",
       "1                                                  bear  \n",
       "2                           bed,book,chair,potted plant  \n",
       "3                                   car,stop sign,truck  \n",
       "4                                        bed,teddy bear  \n",
       "...                                                 ...  \n",
       "4995                                  cell phone,person  \n",
       "4996                            bench,person,skateboard  \n",
       "4997                                              clock  \n",
       "4998                                             toilet  \n",
       "4999                                             banana  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xretrieval\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"blip2_captioned_coco_val_2017.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       Available Models                        </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Model ID                                      </span>┃<span style=\"font-weight: bold\"> Model Input </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers/Salesforce/blip2-itm-vit-g       </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text-image  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers/Salesforce/blip2-itm-vit-g-text  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> text        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers/Salesforce/blip2-itm-vit-g-image </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> image       </span>│\n",
       "└───────────────────────────────────────────────┴─────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                       Available Models                        \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mModel ID                                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel Input\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers/Salesforce/blip2-itm-vit-g      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext-image \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers/Salesforce/blip2-itm-vit-g-text \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtext       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers/Salesforce/blip2-itm-vit-g-image\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mimage      \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────────────────────────┴─────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xretrieval.list_models(\"blip2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 12:56:56.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxretrieval.core\u001b[0m:\u001b[36mrun_benchmark\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mEncoding database text for transformers/Salesforce/blip2-itm-vit-g\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ac6be5a1924be0a17c9bec130d736b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding captions:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "\u001b[32m2024-11-20 12:56:58.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxretrieval.core\u001b[0m:\u001b[36mrun_benchmark\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mEncoding query text for transformers/Salesforce/blip2-itm-vit-g\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740aa2c83a394bf386fcbc306dba5ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding captions:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d5030b1ceb42b6b6032541c79b2bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.2617,\n",
       " 'NormalizedDCG': 0.2874,\n",
       " 'Precision': 0.1589,\n",
       " 'Recall': 0.3622,\n",
       " 'HitRate': 0.3622,\n",
       " 'MAP': 0.2556}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xretrieval.run_benchmark(\n",
    "    dataset=df,\n",
    "    model_id=\"transformers/Salesforce/blip2-itm-vit-g\",\n",
    "    mode=\"text-to-text\",\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 12:57:04.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxretrieval.core\u001b[0m:\u001b[36mrun_benchmark\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mEncoding database images for transformers/Salesforce/blip2-itm-vit-g\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ad365182354f5a8a762a43883f1ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding images:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 12:58:26.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxretrieval.core\u001b[0m:\u001b[36mrun_benchmark\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mEncoding query text for transformers/Salesforce/blip2-itm-vit-g\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6989c6ba2e4e38847a0cb6395dea30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding captions:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.3999,\n",
       " 'NormalizedDCG': 0.5039,\n",
       " 'Precision': 0.276,\n",
       " 'Recall': 0.7264,\n",
       " 'HitRate': 0.7264,\n",
       " 'MAP': 0.3882}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xretrieval.run_benchmark(\n",
    "    dataset=df,\n",
    "    model_id=\"transformers/Salesforce/blip2-itm-vit-g\",\n",
    "    mode=\"text-to-image\",\n",
    "    top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xretrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
